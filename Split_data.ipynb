{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "itarUpqQtwhX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Processando: flight_data_consolidated.parquet ---\n",
            "Dividindo flight_data_consolidated.parquet (14825727 linhas) em 3 partes de ~4941909 linhas.\n",
            "-> Salvo parte 1 em: f:\\Meus_Projetos\\hackathon\\FlightOnTime\\flight-delay-ds\\data\\consolidated_part_1.parquet. Tamanho: 33.73 MB\n",
            "-> Salvo parte 2 em: f:\\Meus_Projetos\\hackathon\\FlightOnTime\\flight-delay-ds\\data\\consolidated_part_2.parquet. Tamanho: 33.75 MB\n",
            "-> Salvo parte 3 em: f:\\Meus_Projetos\\hackathon\\FlightOnTime\\flight-delay-ds\\data\\consolidated_part_3.parquet. Tamanho: 33.65 MB\n",
            "Arquivo original flight_data_consolidated.parquet deletado.\n",
            "--- Processando: flight_data_with_features.parquet ---\n",
            "Dividindo flight_data_with_features.parquet (11408131 linhas) em 2 partes de ~5704065 linhas.\n",
            "-> Salvo parte 1 em: f:\\Meus_Projetos\\hackathon\\FlightOnTime\\flight-delay-ds\\data\\features_part_1.parquet. Tamanho: 88.78 MB\n",
            "-> Salvo parte 2 em: f:\\Meus_Projetos\\hackathon\\FlightOnTime\\flight-delay-ds\\data\\features_part_2.parquet. Tamanho: 97.62 MB\n",
            "Arquivo original flight_data_with_features.parquet deletado.\n",
            "\n",
            "--- Processando Modelo: random_forest_full_model.pkl ---\n",
            "Dividindo modelo random_forest_full_model.pkl (195.91 MB) em 3 partes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Everton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\Everton\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "dump() got an unexpected keyword argument 'max_bytes'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m split_parquet_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflight_data_with_features.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, num_parts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 3. Modelo de 200 MB (Será dividido automaticamente pelo joblib)\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43msplit_pkl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom_forest_full_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrf_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processo de divisão concluído. Arquivos originais deletados. ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[1], line 78\u001b[0m, in \u001b[0;36msplit_pkl_model\u001b[1;34m(file_name, output_prefix)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# A função dump do joblib permite a divisão automática\u001b[39;00m\n\u001b[0;32m     76\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODELS_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_part\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Carrega o modelo\u001b[39;49;00m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Nível de compressão\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Evita problemas de compatibilidade\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMAX_SIZE_MB\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Tamanho máximo em bytes\u001b[39;49;00m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Verifica os arquivos criados\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(MODELS_DIR):\n",
            "\u001b[1;31mTypeError\u001b[0m: dump() got an unexpected keyword argument 'max_bytes'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "import math\n",
        "\n",
        "# --- Configuração de Caminhos e Limites ---\n",
        "BASE_DIR = os.getcwd() # Deve ser o diretório 'flight-delay-ds'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
        "MAX_SIZE_MB = 95 # Margem de segurança abaixo de 100 MB\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. FUNÇÃO PARA DIVIDIR DATAFRAMES (PARQUET)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def split_parquet_file(file_name, output_prefix, num_parts=None):\n",
        "    \"\"\"Carrega um arquivo Parquet e o divide em partes menores que 95MB.\"\"\"\n",
        "    print(f\"--- Processando: {file_name} ---\")\n",
        "    input_path = os.path.join(DATA_DIR, file_name)\n",
        "\n",
        "    # Verifica o tamanho do arquivo em MB\n",
        "    file_size_mb = os.path.getsize(input_path) / (1024 * 1024)\n",
        "\n",
        "    if file_size_mb <= MAX_SIZE_MB:\n",
        "        print(f\"Arquivo {file_name} ({file_size_mb:.2f} MB) já está abaixo do limite. Ignorando.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_parquet(input_path)\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Calcula o número de partes necessárias com base no tamanho\n",
        "    if num_parts is None:\n",
        "        num_parts = math.ceil(file_size_mb / MAX_SIZE_MB)\n",
        "\n",
        "    chunk_size = total_rows // num_parts\n",
        "    print(f\"Dividindo {file_name} ({total_rows} linhas) em {num_parts} partes de ~{chunk_size} linhas.\")\n",
        "\n",
        "    for i in range(num_parts):\n",
        "        start = i * chunk_size\n",
        "        end = (i + 1) * chunk_size if i < num_parts - 1 else total_rows\n",
        "        chunk = df.iloc[start:end]\n",
        "\n",
        "        output_path = os.path.join(DATA_DIR, f'{output_prefix}_part_{i+1}.parquet')\n",
        "        chunk.to_parquet(output_path, index=False)\n",
        "        print(f\"-> Salvo parte {i+1} em: {output_path}. Tamanho: {os.path.getsize(output_path) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "    # Deletar o arquivo original\n",
        "    os.remove(input_path)\n",
        "    print(f\"Arquivo original {file_name} deletado.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. FUNÇÃO PARA DIVIDIR MODELOS (PKL)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def split_pkl_model(file_name, output_prefix):\n",
        "    \"\"\"Carrega um modelo .pkl e o divide em partes menores que 95MB.\"\"\"\n",
        "    print(f\"\\n--- Processando Modelo: {file_name} ---\")\n",
        "    input_path = os.path.join(MODELS_DIR, file_name)\n",
        "\n",
        "    # Verifica o tamanho do arquivo em MB\n",
        "    file_size_mb = os.path.getsize(input_path) / (1024 * 1024)\n",
        "\n",
        "    if file_size_mb <= MAX_SIZE_MB:\n",
        "        print(f\"Modelo {file_name} ({file_size_mb:.2f} MB) já está abaixo do limite. Ignorando.\")\n",
        "        return\n",
        "\n",
        "    # O Joblib é excelente para salvar modelos em partes\n",
        "\n",
        "    # Calcula o número de partes necessárias (arredondando para cima)\n",
        "    num_parts = math.ceil(file_size_mb / MAX_SIZE_MB)\n",
        "\n",
        "    print(f\"Dividindo modelo {file_name} ({file_size_mb:.2f} MB) em {num_parts} partes.\")\n",
        "\n",
        "    # A função dump do joblib permite a divisão automática\n",
        "    output_path = os.path.join(MODELS_DIR, f'{output_prefix}_part')\n",
        "\n",
        "    joblib.dump(\n",
        "        joblib.load(input_path), # Carrega o modelo\n",
        "        output_path,\n",
        "        compress=3, # Nível de compressão\n",
        "        protocol=4, # Evita problemas de compatibilidade\n",
        "        max_bytes=int(MAX_SIZE_MB * 1024 * 1024) # Tamanho máximo em bytes\n",
        "    )\n",
        "\n",
        "    # Verifica os arquivos criados\n",
        "    for f in os.listdir(MODELS_DIR):\n",
        "        if f.startswith(f'{output_prefix}_part'):\n",
        "            print(f\"-> Salvo parte em: {f}. Tamanho: {os.path.getsize(os.path.join(MODELS_DIR, f)) / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "    # Deletar o arquivo original\n",
        "    os.remove(input_path)\n",
        "    print(f\"Modelo original {file_name} deletado.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. EXECUÇÃO PRINCIPAL\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# 1. Arquivo de 181 MB (Exige 3 partes para ficar abaixo de 95MB)\n",
        "split_parquet_file('flight_data_consolidated.parquet', 'consolidated', num_parts=3)\n",
        "\n",
        "# 2. Arquivo de 105 MB (Exige 2 partes para ficar abaixo de 95MB)\n",
        "split_parquet_file('flight_data_with_features.parquet', 'features', num_parts=2)\n",
        "\n",
        "# 3. Modelo de 200 MB (Será dividido automaticamente pelo joblib)\n",
        "split_pkl_model('random_forest_full_model.pkl', 'rf_model')\n",
        "\n",
        "print(\"\\n--- Processo de divisão concluído. Arquivos originais deletados. ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
